<div id="content" class="page-content" itemprop="articleBody">

    <h3>Background:</h3>
    <p>
        While a student at UW Madison, I developed an extensive working relationship with the <a href="https://eliceirilab.org/">Laboratory for Optical and Computational Imagery (LOCI)</a>, led by <a href="https://scholar.google.com/citations?user=W86WgnQAAAAJ&hl=en&oi=ao">Kevin Eliceiri</a>. LOCI conducts applied optics research involving the engineering of novel microscopy techniques for the purpose of furthering cell biology research. My work with LOCI spanned a variety of projects anchored around the central theme of creating tools for explainability and interpretability of the high dimensional microscopy datasets generated by bleeding edge optical methods utilized at the Lab<br><br>
        The culmination of my work at LOCI was the development of a data sonification tool which enabled scientists to analyze previously inaccessible features of extremely high dimensional hyperspectral microscopy datasets. I demonstrated how this tool can be used to explore and discover interesting spectral characteristics in datasets where traditional visualization methods fail, and published these findings in <a href="https://f1000research.com/articles/5-2572/v1">F1000 as a methods paper.</a><br><br>
        My other work with LOCI can be seen <a href="/projects/microanimations">here</a><br><br>
        My collaborator on this project, <a href="https://scholar.google.com/citations?user=sLyD5csAAAAJ&hl=en">Andreas Velten</a>, built a microscope that collects extremely high dimensional fluorescence microscopy datasets. His <a href="https://ieeexplore.ieee.org/document/6833014">Hyperspectral Swept Field Confocal Microscope</a> is used at LOCI for imaging transgenic specimens that contain many fluorescent cellular markers--it has been successfully calibrated to collect high resolution image volumes with up to 17 spectral channels. <br><br>
        <figure>
            <img src="/content/projects/hyperspectralmicro/images/hyperspectral_veltenscope.png">
            <figcaption>A diagram from Andreas' <a href="https://ieeexplore.ieee.org/document/6833014">paper</a> on the microscope we used for this project</figcaption>
        </figure>
    </p>
    <p>

        Researchers at the bleeding edge of life science are not only interested in fluorescence microscopy as a means of mapping structural characteristics of sub-cellular environments. Recently, there has been a focus on mining high dimensional multispectral fluorescence data for subtle cues about impending cell behavior, such as stem cell differentiation. This means there is a growing demand, both in academia and private industry R&D, for technologies like Andreas’ microscope which can yield very spectrally dense datasets. However, current data visualization methods provide no meaningful way of exploring such spectrally dense data. <br><br>
        This bottleneck exists fundamentally because the human eye only has 3 color cones, and is only spectrally sensitive to a range of frequencies that is far smaller than the spectral range of a 17-D image volume. There is a huge data loss when 17 dimensions have to be transcoded down to 3. The human ear, however, is uniquely suited for perceiving interesting signals in multispectral data. This distinction was our entry point into developing a sonification-based solution to the problem of wrangling the data coming off of Andreas’ scope.<br><br>
        <figure class="half">
            <img src="/content/projects/hyperspectralmicro/images/Hyperspectral-Sonification-900-450-1.png">
            <img src="/content/projects/hyperspectralmicro/images/Hyperspectral-Sonification-900-450-2.png">
            <figcaption>Figures cited in our paper describing the limits of spectral perception. The substantial difference in spectral sensitvitiy between the ear and the eye made sonification an interesting solution to the problem of understanding spectrally dense data</figcaption>
        </figure>

    </p>
    <h3>Process:</h3>
    <p>
        As an alternative to computational dimensionality reduction, we sought out to explore data sonification as a way to directly perceive spectrally rich datasets. By turning the dataset into an interactive soundscape, we theorized it would be possible to make obvious spectral qualities of the data that were invisible to visualization-only methods.<br><br>
    </p>
    <h4>Iteration</h4>
    <p>
        Designing an ideal image-to-sound mapping for our use case involved working with a variety of interesting constraints. We wanted to magnify a variety of different subtle spectral shifts occurring across an image volume while ensuring the overall gamut of sounds contained no harsh zones that would create an unpleasant user experience. This required trying many different synthesis methods to find a happy medium of expressiveness and usability.
        <figure style="text-align: center; width: 70%;">
            <img style="margin-left: 12vw;"src="/content/projects/hyperspectralmicro/images/Hyperspectral-Sonification-900-450-3.png">
            <figcaption>Iterating through image-to-sound mappings in MAX/MSP + Jitter</figcaption>
        </figure>
    </p>
    <p>
        I utilized Max/MSP with Jitter for the broad iterative process of identifying which synth voice characteristics were desirable for our use case. This in itself was a very interesting process which was documented in our paper--we referenced the Sonification Handbook to come up with heuristics for developing sounds which are useful specifically for microscopy image volumes. As part of this effort I also created a calibration system that other developers can use to test the efficacy of their own synth voice definitions on multispectral data.
    </p>
    <h4>Delivering the Product</h4>
    <p>
        After defining a sound space that would sonify our image volume, the next priority was to develop a software product that would encapsulate the user experience of exploring an image volume to generate these sounds. LOCI leads the development of a widely used scientific image processing + analysis toolkit called FIJI, which has a lot of built in support for viewing and interacting with multispectral image volumes. I leveraged this native support by building a FIJI plugin that communicates with Supercollider (an algorithmic sound composition framework) to directly generate our defined sounds by clicking and dragging across the image volume in a FIJI volume viewer.<br><br>
        <figure class="half">
            <img src="/content/projects/hyperspectralmicro/images/Hyperspectral-Sonification-900-450-4.png">
            <img src="/content/projects/hyperspectralmicro/images/Hyperspectral-Sonification-900-450-5.png">
        
            <figcaption>Our final application architecture (seen on the left) was tested against FocalCheck spectral calibration beads (seen on the right)</figcaption>
        </figure>
    </p>
    <!-- <div style="padding:56.56% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/112001811?h=9606a19fbd&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;" title="Arabidopsis Sonification Demo"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script> -->
    <p>
        
        We used this architecture to test the sonification with algorithmically generated multispectral data, FocalCheck multispectral fluorescence beads, and finally an actual transgenic specimen. In all cases we observed the sonification was able to reveal spectral properties of the dataset that were impossible to observe through visualization of the raw image volume.
    </p>
    <h3>Impact:</h3>
    <p>

        This project was an incredible opportunity to make a contribution to the burgeoning field of hyperspectral fluorescence microscopy. Our novel but unorthodox method of analyzing this data space was able to capture subtle but very important features in the hyperspectral data--this underscores the importance of creative tool building practices in computational life science. Furthermore, by creating an extensible software architecture, we’ve enabled future developers to easily create their own sonifications for hyperspectral datasets. LOCI has made the source code of this project public <a href="https://github.com/uw-loci/sonification">here</a>
    </p>



    <hr>
    <!-- <footer class="page-footer">
        <div class="inline-btn">
<a class="btn-social twitter" href="https://twitter.com/intent/tweet?text=3D%20Genome&amp;url=https://invizibility.github.io/work/3D-Genome/&amp;via=invizibility" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i> Share on Twitter</a>
<a class="btn-social facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://invizibility.github.io/work/3D-Genome/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i> Share on Facebook</a>
<a class="btn-social google-plus" href="https://plus.google.com/share?url=https://invizibility.github.io/work/3D-Genome/" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i> Share on Google+</a>
</div>
/.share-this

        
    </footer> -->
    <!-- /.footer -->
    <aside>
        
    </aside>
</div>